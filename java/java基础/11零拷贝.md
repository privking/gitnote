# 零拷贝

## DMA技术

DMA:**直接内存访问（Direct Memory Access）**

**在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务**



没有用dma技术时：

- CPU 发出对应的指令给磁盘控制器，然后返回；
- 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个**中断**；
- CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 **CPU 是无法执行其他任务的**。

![img](https://raw.githubusercontent.com/privking/king-note-images/master/img/note/640-1605606432-94ac89.webp)

有dma后

- 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；
- 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；
- DMA 进一步将 I/O 请求发送给磁盘；
- 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到**磁盘控制器**的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；
- **DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务**；
- 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；
- CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；

![img](https://raw.githubusercontent.com/privking/king-note-images/master/img/note/640-1605606559-13c542.webp)

## 传统文件拷贝

![img](https://raw.githubusercontent.com/privking/king-note-images/master/img/note/640-1605606629-536074.webp)

上下文切换**成本并不小**，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能

4次数据拷贝

- *第一次拷贝*，把磁盘上的**数据**拷贝到操作系统**内核的缓冲区**里，这个拷贝的过程是通过 DMA 搬运的。
- *第二次拷贝*，把**内核缓冲区**的数据拷贝到**用户的缓冲区**里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。
- *第三次拷贝*，把刚才拷贝到**用户的缓冲区**里的数据，再拷贝到内核的 **socket 的缓冲区**里，这个过程依然还是由 CPU 搬运的。
- *第四次拷贝*，把内核的 **socket 缓冲区**里的数据，拷贝到**网卡的缓冲**区里，这个过程又是由 DMA 搬运的

## 实现零拷贝

### mmp+write

```
buf = mmap(file, len);
write(sockfd, buf, len);
```

`mmap()` 系统调用函数会直接把内核缓冲区里的数据「**映射**」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作

具体过程如下：

- 应用进程调用了 `mmap()` 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。

![img](https://raw.githubusercontent.com/privking/king-note-images/master/img/note/640-1605606969-a78059.webp)

#### sendfile

```
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

前两个参数分别是**目的端**和**源端**的**文件描述符**，后面两个参数是源端的**偏移量**和复制数据的**长度**，返回值是实际**复制数据的长度**

该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。

![img](https://raw.githubusercontent.com/privking/king-note-images/master/img/note/640-1605607120-baf9fe.webp)

### 网卡支持 SG-DMA（*The Scatter-Gather Direct Memory Access*）

```
$ ethtool -k eth0 | grep scatter-gather
scatter-gather: on
```

支持网卡支持 SG-DMA 技术的情况下， `sendfile()` 系统调用的过程发生了点变化

- 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；
- 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据![img](https://raw.githubusercontent.com/privking/king-note-images/master/img/note/640-1605607256-ab786c.webp)从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；

## 零拷贝技术应用

### kafka

Linux 系统支持 `sendfile()` 系统调用，那么 `transferTo()` 实际上最后就会使用到 `sendfile()` 系统调用函数

```java
@Overridepublic 
long transferFrom(FileChannel fileChannel, long position, long count) throws IOException { 
  return fileChannel.transferTo(position, count, socketChannel);
}
```

### Nginx

Nginx 也支持零拷贝技术，一般**默认是开启**零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：

```
http {
...
    sendfile on
...
}
```

## PageCache

磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是**磁盘高速缓存（PageCache）**

Page cache由**内存**中的物理page组成，其内容对应磁盘上的block。page cache的大小是动态变化的，可以扩大，也可以在内存不足时缩小。cache缓存的存储设备被称为后备存储（backing store），一个page通常包含多个block，这些block不一定是连续的。

### 读Cache

当内核发起一个读请求时（例如进程发起read()请求），首先会检查请求的数据是否缓存到了page cache中，如果有，那么直接从内存中读取，不需要访问磁盘，这被称为**cache命中**（cache hit）。如果cache中没有请求的数据，即**cache未命中**（cache miss），就必须从磁盘中读取数据。然后内核将读取的数据**缓存到cache中**，这样后续的读请求就可以命中cache了。page可以只缓存一个文件**部分的内容**，不需要把整个文件都缓存进来。

### 写Cache

当内核发起一个写请求时（例如进程发起write()请求），同样是**直接往cache中写入**，后备存储中的内容不会直接更新。**内核会将被写入的page标记为dirty，并将其加入dirty list中。内核会周期性地将dirty list中的page写回到磁盘上**，从而使磁盘上的数据和内存中缓存的数据一致。

### Cache回收

Page cache的另一个重要工作是释放page，从而释放内存空间。cache回收的任务是选择合适的page释放，并且如果page是dirty的，需要将page写回到磁盘中再释放。理想的做法是释放距离下次访问时间最久的page，但是很明显，这是不现实的。下面先介绍LRU算法，然后介绍基于LRU改进的Two-List策略，后者是Linux使用的策略。

### Two-List策略

Two-List策略维护了两个list，**active list 和 inactive list**。在active list上的page被认为是hot的，不能释放。只有inactive list上的page可以被释放的。首次缓存的数据的page会被加入到inactive list中，已经在**inactive list中的page如果再次被访问，就会移入active list中**。**两个链表都使用了伪LRU算法维护**，新的page从尾部加入，移除时从头部移除，就像队列一样。如果active list中page的数量远大于inactive list，那么active list头部的页面会被移入inactive list中，从而位置两个表的平衡。

### 预读功能

还有一点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，**PageCache 使用了「预读功能」**。

假设 read 方法每次只会读 `32 KB` 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低，如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。

### 大文件传输

传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DRM 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能

## 大文件传输

在高并发的场景下，针对大文件的传输的方式，应该使用「**异步 I/O** + **直接 I/O**」来替代零拷贝技术

内核向磁盘发起读请求，但是可以**不等待数据就位就可以返回**，于是进程此时可以处理其他任务

内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的**通知**，再去处理数据

绕开 PageCache 的 I/O 叫直接 I/O

```
location /video/ { 
    sendfile on; 
    aio on; 
    directio 1024m; 
}
```

