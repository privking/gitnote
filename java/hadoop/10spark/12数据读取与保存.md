# 数据读取与保存

从文件中读取数据是创建 RDD 的一种方式.

把数据保存的文件中的操作是一种 Action.

Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。

文件格式分为：Text文件、Json文件、csv文件、Sequence文件以及Object文件；

文件系统分为：本地文件系统、HDFS、Hbase 以及 数据库。

平时用的比较多的就是: 从 HDFS 读取和保存 Text 文件.

## 读写Text文件

```scala
val rdd1 = sc.textFile("./words.txt")

rdd2.saveAsTextFile("hdfs://hadoop201:9000/words_output")
```

## 读写JSON文件

如果 JSON 文件中每一行就是一个 JSON 记录，那么可以通过将 JSON 文件当做文本文件来读取，然后利用相关的 JSON 库对每一条数据进行 JSON 解析。

```scala
val rdd1 = sc.textFile("resources/people.json")
import scala.util.parsing.json.JSON
val rdd2 = rdd1.map(JSON.parseFull)
rdd2.collect
//res2: Array[Option[Any]] = Array(Some(Map(name -> Michael)), Some(Map(name -> Andy, age -> 30.0)), Some(Map(name -> Justin, age -> 19.0)))

```

## 读写SequenceFile

 SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对而设计的一种平面文件(Flat File)。

Spark 有专门用来读取 SequenceFile 的接口。

```scala
 val rdd1 = sc.parallelize(Array(("a", 1),("b", 2),("c", 3)))
 rdd1.saveAsSequenceFile("hdfs://hadoop201:9000/seqFiles")
 
 val rdd1 = sc.sequenceFile[String, Int]("hdfs://hadoop201:9000/seqFiles")
  rdd1.collect
//res4: Array[(String, Int)] = Array((a,1), (b,2), (c,3))

```

## 读写objectFile文件

对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制。

```scala
 val rdd1 = sc.parallelize(Array(("a", 1),("b", 2),("c", 3)))
rdd1.saveAsObjectFile("hdfs://hadoop201:9000/obj_file")

 val rdd1 = sc.objectFile[(String, Int)]("hdfs://hadoop201:9000/obj_file")
rdd1.collect
/res8: Array[(String, Int)] = Array((a,1), (b,2), (c,3))

```

